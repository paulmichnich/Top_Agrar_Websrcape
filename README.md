# Top_Agrar_Websrcape
 Webscraping Exercise for the Top Agrar Website

First preliminary (baby)steps and results:
![keyword_occurrences_facet_plot](https://github.com/user-attachments/assets/2952ccd5-03b3-45f5-b055-8cddc768e31f)

# Code: 01_Title_Webscrape_Top_Agrar.R

The R script automates the web scraping of article titles from the TopAgrar website's archival issues (referred to as "Heftausgabe") between 2000 and 2025. It systematically navigates through yearly archive pages, extracts issue links, and retrieves article titles from each individual issue page. The script uses the rvest library for web scraping and dplyr for data manipulation, ensuring efficient handling of data structures.

The process begins by initializing necessary libraries and defining variables, such as the base URL of the TopAgrar website and a save path for the output files. For each year, the script constructs the corresponding URL for the archive and scrapes the page for links to "Heftausgabe" issues. The links are filtered and, if necessary, converted from relative to absolute URLs for compatibility.

Each issue page is then accessed, and the article titles are extracted. These titles, along with their associated URLs and publication years, are stored in a structured data frame. Any errors encountered during the scraping process, such as inaccessible pages or malformed content, are logged into a separate error log, allowing for easy debugging.

The final output consists of two CSV files: one containing all successfully scraped article titles, including their years and issue URLs, and another containing details of all encountered errors. This ensures both the primary data and debugging information are preserved.

The script is designed to handle unexpected issues gracefully using tryCatch blocks, making it robust for large-scale web scraping tasks. It is flexible enough to adjust for changes in the website's structure by modifying the HTML node selectors. This tool is invaluable for aggregating data from large archives while maintaining detailed records of the scraping process and any issues.


# Code: 01_Sentiment_Code.R

The R script analyzes the prevalence of climate- and weather-related keywords in article titles scraped from TopAgrar archives. It employs text processing and visualization to explore trends in keyword occurrences over time, making it an effective tool for understanding thematic shifts in agricultural publications.

The process begins by loading the necessary libraries and reading in the CSV file containing article titles. A predefined list of German keywords relevant to climate change and weather is used to search through the article titles. Each title is converted to lowercase to ensure case-insensitive matching. The script then checks for the presence of each keyword in every title, storing the results in a matrix that is later converted into a data frame.

The data frame is enriched by adding the year and article title information, allowing for a temporal analysis of keyword occurrences. A yearly summary is then generated by aggregating keyword counts for each year. The data is reshaped into a long format, enabling flexible plotting with ggplot2.

Two visualizations are created:

- Overall Keyword Trends: This line plot displays the annual frequency of all keywords on a single graph, with each keyword represented by a distinct line and color. It provides a comprehensive view of how the discussion around specific topics evolves over time.
- Faceted Keyword Trends: This faceted plot presents a separate line chart for each keyword, with independent y-axes to highlight individual trends. It allows for a focused analysis of each term's occurrence.

# Misc

Folder structure is designed to be extended with Google Trends analysis

